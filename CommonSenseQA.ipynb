{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csense.benchmark import cqa\n",
    "from PyDictionary import PyDictionary\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "dictionary=PyDictionary()\n",
    "# import en_core_web_sm\n",
    "\n",
    "# nlp = en_core_web_sm.load()\n",
    "# nlp = spacy.load(\"/Users/anushkabaoni/Downloads/Cornell Courses-Spring2020/Advanced AI/CommonsenseQADataset/venv/lib/python3.7/site-packages/en_core_web_sm\")\n",
    "# nlp = spacy.load(\"../CommonsenseQADataset/venv/lib/python3.7/site-packages/en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = cqa.getDataSet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9741\n",
      "what :  5797\n",
      "where :  2863\n",
      "why :  242\n",
      "who :  50\n",
      "how :  216\n",
      "which :  44\n",
      "when :  126\n",
      "9338\n"
     ]
    }
   ],
   "source": [
    "what_questions = []\n",
    "where_questions = []\n",
    "why_questions = []\n",
    "who_questions = []\n",
    "which_questions = []\n",
    "when_questions = []\n",
    "how_questions = []\n",
    "\n",
    "rand = []\n",
    "\n",
    "for question in dataset:\n",
    "    assigned = False\n",
    "    text = set(nltk.word_tokenize(question['question']['stem'].lower()))\n",
    "    if(\"what\" in text and \"where\" not in text and \"why\" not in text and \"who\" not in text and \"when\" not in text and \"how\" not in text and \"which\" not in text):\n",
    "        what_questions.append(question)\n",
    "        continue\n",
    "    if(\"what\" not in text and \"where\" in text and \"why\" not in text and \"who\" not in text and \"when\" not in text and \"how\" not in text and \"which\" not in text):\n",
    "        where_questions.append(question)\n",
    "        continue\n",
    "    if(\"what\" not in text and \"where\" not in text and \"why\" in text and \"who\" not in text and \"when\" not in text and \"how\" not in text and \"which\" not in text):\n",
    "        why_questions.append(question)\n",
    "        continue\n",
    "    if(\"what\" not in text and \"where\" not in text and \"why\" not in text and \"who\" in text and \"when\" not in text and \"how\" not in text and \"which\" not in text):\n",
    "        who_questions.append(question)\n",
    "        continue\n",
    "    if(\"what\" not in text and \"where\" not in text and \"why\" not in text and \"who\" not in text and \"when\" in text and \"how\" not in text and \"which\" not in text):\n",
    "        when_questions.append(question)\n",
    "        continue\n",
    "    if(\"what\" not in text and \"where\" not in text and \"why\" not in text and \"who\" not in text and \"when\" not in text and \"how\" in text and \"which\" not in text):\n",
    "        how_questions.append(question)\n",
    "        continue     \n",
    "    if(\"what\" not in text and \"where\" not in text and \"why\" not in text and \"who\" not in text and \"when\" not in text and \"how\" not in text and \"which\" in text):\n",
    "        which_questions.append(question)\n",
    "        continue        \n",
    "    # whether, whose, whom, how\n",
    "    \n",
    "    doc = nlp(question['question']['stem'].lower())\n",
    "    for token in doc:\n",
    "        if(token.dep_ == 'ROOT'):\n",
    "            childern = [str(child) for child in token.children]         \n",
    "            if(\"what\" in childern):\n",
    "                what_questions.append(question)\n",
    "                assigned=True\n",
    "                break\n",
    "            elif(\"where\" in childern):\n",
    "                where_questions.append(question)\n",
    "                assigned=True\n",
    "                break\n",
    "            elif(\"why\" in childern):\n",
    "                why_questions.append(question)\n",
    "                assigned=True\n",
    "                break\n",
    "            elif(\"who\" in childern):\n",
    "                who_questions.append(question)\n",
    "                assigned=True\n",
    "                break\n",
    "            elif(\"how\" in childern):\n",
    "                how_questions.append(question)\n",
    "                assigned=True\n",
    "                break\n",
    "            elif(\"which\" in childern):\n",
    "                which_questions.append(question)\n",
    "                assigned=True\n",
    "                break\n",
    "            elif(\"when\" in childern):\n",
    "                when_questions.append(question)\n",
    "                assigned=True\n",
    "                break\n",
    "    if not assigned:\n",
    "        rand.append(question)\n",
    "        for token in doc:\n",
    "            if(\"what\" == token.text and 'dobj' == token.dep_):\n",
    "                what_questions.append(question)\n",
    "                break\n",
    "\n",
    "print(len(dataset))\n",
    "print(\"what : \", len(what_questions))\n",
    "print(\"where : \",len(where_questions))\n",
    "print(\"why : \",len(why_questions))\n",
    "print(\"who : \",len(who_questions))\n",
    "print(\"how : \",len(how_questions))\n",
    "print(\"which : \",len(which_questions))\n",
    "print(\"when : \",len(when_questions))\n",
    "print(len(what_questions) + len(where_questions) + len(why_questions) + len(who_questions) + len(how_questions) + len(which_questions) + len(when_questions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root: put,\n",
      " contexts: ['subjects: [\\'name: you, properties: []\\'], action: name: have, properties: [], objects: [\"name: glass, properties: [(\\'_\\', [\\'empty\\'])]\"]', 'subjects: [\\'name: you, properties: []\\'], action: name: put, properties: [], objects: [\"name: glass, properties: [(\\'_\\', [\\'empty\\'])]\"]'],\n",
      " candidates: ['A', 'B'], type: WHERE\n"
     ]
    }
   ],
   "source": [
    "from csense.parser.context import *\n",
    "\n",
    "def getPos(word, doc):\n",
    "    for token in doc:\n",
    "        if(token.text == word):\n",
    "            return token.pos_\n",
    "\n",
    "## TODO \n",
    "# conjuctions of properties \n",
    "# conjuctions of dobj\n",
    "# conjuctions of pobj\n",
    "# conjuctions of subject\n",
    "\n",
    "def parse(doc, questionType, candidates):\n",
    "\n",
    "    question = Question()\n",
    "    question.questionType = questionType\n",
    "    contextMap = {}\n",
    "    entityToPropMap = {}\n",
    "    entityToPrepositionPObjMap = {}\n",
    "    question.candidates = candidates\n",
    "    \n",
    "    # find root \n",
    "    for token in doc:\n",
    "        if(token.dep_ == \"ROOT\"):\n",
    "            question.root = token.text\n",
    "            break\n",
    "\n",
    "    # associate properties\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'amod':\n",
    "            if token.head.text not in entityToPropMap:\n",
    "                entityToPropMap[token.head.text] = []\n",
    "            entityToPropMap[token.head.text].append(str(token.text))\n",
    "            # { \"glass\" : [\"empty\", ..]}\n",
    "            \n",
    "    # associate prepositions\n",
    "    for token in doc:\n",
    "        if token.dep_ == 'prep':\n",
    "            if token.head.text not in entityToPrepositionPObjMap:\n",
    "                entityToPrepositionPObjMap[token.head.text] = []\n",
    "            entities = []\n",
    "            for child in token.children:\n",
    "                if(child.pos_ == \"NOUN\"):\n",
    "                    ent = Subject(child.text, {})\n",
    "                if(child.pos_ == \"VERB\"):\n",
    "                    ent = Action(child.text, {})\n",
    "                # associate amods of preposition entities. (rent in(prep) glove-shaped(amod) state(ent))\n",
    "                if(ent.name in entityToPropMap):\n",
    "                    if(\"_\" not in ent.properties):\n",
    "                        ent.properties[\"_\"] = []\n",
    "                    ent.properties[\"_\"].extend(entityToPropMap[ent.name])\n",
    "                entities.append(ent)\n",
    "            entityToPrepositionPObjMap[token.head.text].append((token.text,entities))\n",
    "            # { \"rent\" : [(in, [state])]}\n",
    "            #.   (sub) : [(prep, [pobj])]\n",
    "    \n",
    "    # find sub - action pairs\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if chunk.root.dep_ == 'nsubj':\n",
    "            sub = Subject(chunk.root.text, {})\n",
    "            # associating prop with subject\n",
    "            if(sub.name in entityToPropMap):\n",
    "                if(\"_\" not in sub.properties):\n",
    "                    sub.properties[\"_\"] = []\n",
    "                sub.properties[\"_\"].extend(entityToPropMap[sub.name])\n",
    "            \n",
    "            # associating prepositions with subject\n",
    "            if(sub.name in entityToPrepositionPObjMap):\n",
    "                for (prep, pobjs) in entityToPrepositionPObjMap[sub.name]:\n",
    "                    if(prep not in sub.properties):\n",
    "                        sub.properties[prep] = []\n",
    "                    sub.properties[prep].extend(pobjs)\n",
    "            \n",
    "            # finding verb\n",
    "            if(getPos(chunk.root.head.text, doc) == 'VERB'):\n",
    "                verb = Action(chunk.root.head.text, {})\n",
    "                \n",
    "                # associating prop with dobject\n",
    "                if(verb.name in entityToPropMap):\n",
    "                    if(\"_\" not in verb.properties):\n",
    "                        verb.properties[\"_\"] = []\n",
    "                    verb.properties[\"_\"].extend(entityToPropMap[verb.name])\n",
    "\n",
    "                # associating prepositions with subject\n",
    "                if(verb.name in entityToPrepositionPObjMap):\n",
    "                    for (prep, pobjs) in entityToPrepositionPObjMap[verb.name]:\n",
    "                        if(prep not in verb.properties):\n",
    "                            verb.properties[prep] = []\n",
    "                        verb.properties[prep].extend(pobjs)\n",
    "                \n",
    "                \n",
    "                \n",
    "                context = Context()\n",
    "                context.action = verb\n",
    "                context.subjects.append(sub)\n",
    "                #todo handle conjunction of subjects\n",
    "                contextMap[verb.name] = context\n",
    "\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        # associate dobjs\n",
    "        if chunk.root.dep_ == 'dobj':\n",
    "            obj = Object(chunk.root.text, {}) \n",
    "            # associating prop with dobject\n",
    "            if(obj.name in entityToPropMap):\n",
    "                if(\"_\" not in obj.properties):\n",
    "                    obj.properties[\"_\"] = []\n",
    "                obj.properties[\"_\"].extend(entityToPropMap[obj.name])\n",
    "\n",
    "            # associating prepositions with subject\n",
    "            if(obj.name in entityToPrepositionPObjMap):\n",
    "                for (prep, pobjs) in entityToPrepositionPObjMap[obj.name]:\n",
    "                    if(prep not in obj.properties):\n",
    "                        obj.properties[prep] = []\n",
    "                    obj.properties[prep].extend(pobjs)\n",
    "                    \n",
    "            if(getPos(chunk.root.head.text, doc) == 'VERB'):\n",
    "                verb = chunk.root.head.text\n",
    "                if(verb in contextMap):\n",
    "                    contextMap[verb].objects.append(obj)\n",
    "\n",
    "            \n",
    "    for key in contextMap:\n",
    "        question.contexts.append(contextMap[key])\n",
    "    \n",
    "    return question            \n",
    "\n",
    "doc = nlp(\"If you have a empty glass and are thirsty, where would you put the glass?\")\n",
    "# doc = nlp(\"If you rent a condominium in a glove-shaped state where are you?\")\n",
    "print(parse(doc, \"WHERE\", [\"A\", \"B\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(where_questions[10]) \n",
    "# doc = nlp(question['question']['stem'].lower())\n",
    "\n",
    "for sent in range(0,len(where_questions)):\n",
    "# for sent in range(0,20):\n",
    "    doc = nlp(where_questions[sent]['question']['stem'])\n",
    "\n",
    "    print(doc)\n",
    "    print(sent)\n",
    "    phrases = getSentence(doc)\n",
    "    for token in doc:\n",
    "        if(token.dep_ == \"ROOT\"):\n",
    "            root = token.text\n",
    "    print(root)\n",
    "    print(\"################\")\n",
    "\n",
    "sent = where_questions[10]['question']['stem']\n",
    "doc = nlp(sent.lower())\n",
    "doc = nlp(\"If you have a empty glass and are thirsty, where would you put the glass?\")\n",
    "# doc = nlp(\"Where could you see an advertisement while reading news?\")\n",
    "# getParsedSentence(doc)\n",
    "getSentence(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSentence(doc):\n",
    "    phrase = []\n",
    "    sub = []\n",
    "    verb = []\n",
    "    obj = []\n",
    "    phrases = []\n",
    "\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if chunk.root.dep_ == 'nsubj':\n",
    "            \n",
    "            # if len(sub) == 1:               # tackle 2 subjects one after the other\n",
    "            #     phrases.append(phrase)\n",
    "            #     sub = []\n",
    "            #     verb = []\n",
    "            #     obj = []\n",
    "            #     phrase = []\n",
    "\n",
    "            # sub = chunk.root.text\n",
    "\n",
    "            sub =  chunk.text\n",
    "            verb = chunk.root.head.text\n",
    "            phrase = []\n",
    "            phrase.append(sub)\n",
    "            phrase.append(verb)\n",
    "\n",
    "        elif chunk.root.dep_ == 'dobj' or chunk.root.dep_ == 'pobj':\n",
    "\n",
    "            # if len(obj) == 1:\n",
    "            #     phrase.append(obj)\n",
    "            #     phrases.append(phrase)\n",
    "            #     verb = []\n",
    "            #     obj = []\n",
    "            #     phrase = phrase[0:1]\n",
    "\n",
    "            obj = chunk.text\n",
    "\n",
    "            verb = chunk.root.head.text\n",
    "            if len(phrase) == 2 and phrase[1] != verb:\n",
    "                phrase[1] += \" \" + verb\n",
    "            elif len(phrase) ==1:\n",
    "                phrase.append(verb)\n",
    "            \n",
    "            phrase.append(obj)\n",
    "\n",
    "            # phrases.append(phrase)\n",
    "            # obj = []\n",
    "            # verb = []\n",
    "            # phrase = phrase[0:1]\n",
    "\n",
    "        if len(phrase) == 3:\n",
    "            phrases.append(phrase)\n",
    "            verb = []\n",
    "            obj = []\n",
    "            phrase = phrase[0:1]\n",
    "\n",
    "        # print(chunk.text, \"+\", chunk.root.text, \"+\", chunk.root.dep_, \"+\", chunk.root.head.text)\n",
    "\n",
    "    # if len(phrase):\n",
    "    #     phrases.append(phrase)\n",
    "    print(\"PHRASES\")\n",
    "    print(phrases)\n",
    "    return phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy.explain('pobj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1 = nlp(where_questions[4]['question']['stem'])\n",
    "doc2 = nlp(where_questions[6]['question']['stem'])\n",
    "doc3 = nlp(where_questions[9]['question']['stem'])\n",
    "doc4 = nlp(where_questions[12]['question']['stem'])\n",
    "doc5 = nlp(where_questions[13]['question']['stem'])\n",
    "doc6 = nlp(where_questions[16]['question']['stem'])\n",
    "doc7 = nlp(where_questions[17]['question']['stem'])\n",
    "doc8 = nlp(where_questions[18]['question']['stem'])\n",
    "doc9 = nlp(where_questions[19]['question']['stem'])\n",
    "doc10 = nlp(where_questions[2734]['question']['stem'])\n",
    "doc11 = nlp(where_questions[2735]['question']['stem'])\n",
    "doc12 = nlp(where_questions[2743]['question']['stem'])\n",
    "doc13 = \"If you have a empty glass and are thirsty, where would you put the glass?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(doc12)\n",
    "phrases = getSentence(doc12)\n",
    "# print(phrases)\n",
    "print(where_questions[2743])\n",
    "for token in doc12:\n",
    "    if(token.dep_ == \"ROOT\"):\n",
    "        root = token.text\n",
    "print(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def lookup(word):\n",
    "    url = \"http://conceptnet5.media.mit.edu/data/5.4/c/en/\" + word\n",
    "    obj = requests.get(url).json()\n",
    "    print(obj)\n",
    "    for edge in obj['edges']:\n",
    "        meaning = edge['end']['@id'].split('/')\n",
    "        print(meaning[-1])\n",
    "        # print(edge['end']['@id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
